# Hypotheses and Experimentation

If we want to start using the frictionless, infinite possibilities of the Digital Realm to our advantage, or in other words, if we want to be genuinely small-a-agile, we need to *embrace uncertainty*. And that means *getting all scientific* - not "delivering outputs", not *even* "causing outcomes", in fact, but "testing hypotheses of value" to see which ones move the needle on our Key Results the most - the ones that are most likely to cause the outcome.

There's a big caveat to all of this good stuff: to experiment properly you *have to be good at experimenting*. I.e.: you should have at least an affinity with scientific methods. It's quite easy to conduct "experiments" that simply exist to confirm your biases, so an inability to at least try and remain objective about results, and failure to acknowledge that experiments that turn out exactly as expected are as rare as hens' teeth, put all the good intentions behind experimentation at risk.

Never forget: a *successful* experiment is one that turns out unexpectedly. To know this is to work in the mode of "Falsifiability" - that the odds your idea is right are pretty-much always outweighed by the chance it's wrong, so it's much less effort to go looking for proof it ain't going to work. Don't blame me, blame [Sir Karl Popper](https://en.wikipedia.org/wiki/Falsifiability). This is the kind of stuff you learn at PhD school - no good thesis is complete without a paragraph on "induction vs deduction" and all that jazz. Popper's simple example of falsifiability involved the hypothesis "all swans are white" - which to verify would involve checking *all* the swans. At which point, it's much easier just to find a black one and disprove the statement.

So in other words, it's much less effort for a team to recognise their biases and then going looking for the proof that those are wrong, than to just try and confirm them. You're always going to be able to confirm them, that's the point - but doing so rarely takes you anywhere good. OKRs are intended to help here, because the KR bit gives you the needle you need to move, and that's all a lot more objective than just "we love the stuff we built, it's bound to work, because we're so clever, innit?"

## The Dreaded MVP

I scratched the surface of the Digital Realm's most poorly-understood-concept in [article 2](./Agile_AntiPatterns.md). To sum-up, I described them as Maximally-Non-Viable-Projects, which is what they usually revert to in the hands of people that don't get the point of them (i.e. practically everyone).

My suggestion is to actually think of them as "Unit Tests of Value" (not my idea, wish I could find the person who said it first)... I.e. the (genuine) minimum amount of work you need to do to *disprove* something is going to work. One "MVP" equivalent of this, as a very first test, is to look at your grand product idea, then lean on the Business Model Canvas to hypothesise a market segment that will have struggles your product is going to solve. Then the test is to try and list 20 people or organisations that are strong candidates for having that problem. If you get stuck around ten, your idea might not be as great as you thought. If you do this work *with an Engineer and a Designer* present, they'll be able to get creative with the new information you've gained from this market research and suggest ways of tweaking the idea until you do have 20. (Call this "pivoting" if you like. The Engineer might want to call it "changing the first Unit Test of Value until it passes", if they know what they're doing.)

(As I say - I didn't invent this. I suspect it might have come from Google's [Design Sprints](https://www.thesprintbook.com/the-design-sprint).)

That's MVP one done, by lunchtime on the first day. It's also the first bad idea of the nine-out-of-ten bad ones put to bed, too. Well done! Have a guess what MVP two might be? That's right - coming up with an elevator-pitch-sized description of the *problem* you think your tweaked idea is going to solve, then calling the 20 people / companies you identified in test 1. Can't find six that are interested? (Or indeed, can't find one in the first three that doesn't immediately tell you to clear off?) Guess what - your idea still isn't right - *pivot again*.

Remember to ask the question right, though... So it's not: "...what do you think of this great idea of ours?" It's "...we think we've found a solution for this terrible problem. Is that a problem you struggle with? If so, could you spare some time to walk us through the last time it happened?" If you get outright "no it's never happened to me and I don't care" from more than three quarters of the people you speak to - probably best to consider canning the idea outright.

Anyway - that's two MVPs done by the end of day one, and a whole combo of Product Manager, Designer and Engineer that understand the customers' struggles about 5000% better than they did at the start of the day. If the idea's got legs then you should have also booked-in time with 6 people / organisations that struggle enough with the problem you think you can solve to be willing to walk you through it. Establishing a timeline of the problem with the first three on your list - no solutions, just take them back to the time they last struggled and stepping through it - is MVP number 3.

If you're doing it right, MVP 3 will be a product that reframes the struggle in your *customers' language*, not yours. But you should still only be talking to potential customers about *the problem* at this stage. Use the next three sessions you booked to validate the language you've reframed things in.

If you've hit a really good idea then you ought to get through six sessions in a couple of weeks, and still have half-a-dozen people who are willing, nay eager, to see what you come up with in terms of a solution. Of course, it's unlikely to go this smoothly, and you'll have tweaked and pivoted a good few more times, or more likely you'll have disproved that your idea is going to work by now. But if its still got legs, and you've got "the struggle" mapped out and timelined in the customers' language, then you can start thinking about prototyping a solution (the designer and engineer will probably not have been able to stop themselves from doing this by now, anyway, if you really have landed on a genuine problem to solve).

Then MVPs 10 - 30 are a case of taking the solutions back to the market again, once again walking through the timeline of the struggle, and testing how the solutions line up with it. In [Dave Thomas's terms](https://youtu.be/bEMg9XXIcew?si=WMmJucZFqZma3xt0) - these are the smallest steps you can take with an idea before you get feedback - and (important point), small-a-agility can simply mean "backing-up to the previous step if you find out you've done the last bit wrong". If you're ever doing so much work that you'd be upset to can it when it's wrong, then your steps are too big, and you've fallen too in love with a particular idea.

You'll note that this is all very "start-uppy" so far - but this approach can (and should) be applied to any suggested change to a Product. The difference with existing products is that you should already have a market segment in mind, and a set of customer relationships in place, so finding people to test the problem your change is intended to solve *ought* to be easier.

If you *don't* have a well-understood set of market segments your Products are designed for, and no intimate understanding of the jobs your products support, and the struggles your customers suffer, then get those in place **NOW**, as your *business is a risk until you do*. Any changes you put in without that understanding are 90% likely to be bad ones, that'll disturb the delicate balance of the value your products are somehow delivering by accident.

## Discover and Deliver in the Same Process

There's a recurring theme in modern software production that is fundamentally linked to the core Digital Realm concepts of infinite possibilities and lack of friction. The theme concerns the level of confidence we have in our ideas, which can also be thought of as the degree of uncertainty surrounding them.

Obviously, how sure we are about things is not a particularly *digital* idea - it's more fundamental and abstract than that. The digital bit is that, because of the lack of friction in particular, we can potentially *do more with* our lack of certainty: we can acknowledge it more fully and explore "what we don't know" more comprehensively. This is the concept of "discovery", referred to in a few places above: Marty Cagan's *Product Discovery*, and Steve Blank's *Customer Discovery*, for instance. This is a big topic, so below is just a brief overview comparing three approaches from three different sources already mentioned above:

1. Itamar Gilad's GIST Framework
2. Lean Value Trees (LVTs) and Measures of Success (MoSes), as described in Highsmith, Luu and Robinson's book Edge: Value-Driven Digital Transformation
3. What Bob Moesta describes as: *The 5 bedrock skills of innovators and entrepreneurs*, in his book [Learning To Build](https://www.learningtobuildbook.com).

How the first two help planning and management by arranging strategy, tactics and tasks hierarchically was introduced above. We also met Bob earlier, too, in the sections about Customer Discovery and Jobs To Be Done, where he introduced the idea of "Struggling Moments". But Bob's got more in his locker than just that.

## Goals and Ideas

Itamar's approach to uncertainty is all about "confidence levels". Once we're decided upon the Goal we're trying to reach, defined in terms of the value we're trying to deliver to the market, and quantified in the Key Results we think represent those Goals, we can start to consider the *Ideas* we're going to launch to get to the Goal. There are three important aspects of all ideas (not just Itamar's Capital-I-Ideas) that we should always remember:

1. *Most of them aren't any good*.
2. ... however, if you reject one that *seems bad*, but then a competitor goes off and makes 10 billion Simolians with it, that can be career-limiting.
3. The Digital Realm lets us test lots of ideas, quickly (at least in theory).

So GIST provides a framework for the evaluation of Ideas, the processes of testing quickly, and a methodical way of knowing where each Idea stands in the big picture. Itamar calls that approach *ICE*: Impact, Confidence and Ease, though confesses it should be "IEC", because "Confidence" means "how Confident are we that we've got Impact and Ease right?"

Impact and Ease map nicely onto Marty Cagan's ideas:

* Viability maps directly onto Impact - how well is this going to land in the market? How much is it going to move our Key Result needles? How much value is it going to deliver.
* Usability / Desirability kind of fits between Impact and Ease... How well is this Idea going to resonate with the market? Does it map clearly onto the problems we're trying to solve? Does it all flow nicely from our organisation into the minds of customers and / or users?
* Feasibility maps directly onto Ease - how much work are we going to have to do to deliver it? How closely does it align with technologies we're already masters of?

As for Confidence, Itamar's built a thing called "the Confidence Meter", which is a tool for assessing whereabouts each Idea stands currently on a spectrum of "we know *nothing*" to "we're so all-over this problem that we've built a solution to it that we're virtually certain is going to work". Every step on the road between both ends of this spectrum represents some form of testing that can be performed with your market, with things like "we've come up with a pitch deck that a Sales Person thinks might resonate with customers" at the "we know nothing" end, and "we've got two versions of the UI deployed in an A/B experiment and are monitoring how well each affects these Key Results" at the "we're pretty confident" end. As mentioned, we never really get to "supremely confident", though, because the market is a slippery old thing in the Digital Realm.

## Making Bets

Highsmith et al's framework of Lean Value Trees appears to be very similar to GIST, but there are some fundamental differences. One of those is that the two hierarchies don't map exactly onto each other. GIST's hierarchy supports the entire process of taking strategy into a working product: Goals relate to the very highest, "North Star Metric" level, and Tasks correspond to actual work items in a backlog for a delivery team to chew through. In Highsmith's model, "Vision" sits at the very top, and the "leaves" of the Lean Value Tree are "Initiatives", which are still one level away from any actual work that a delivery team might do.

One benefit of the slightly "higher" level of abstraction of the Lean Value Tree over GIST is that it sits it closer to the process of funding initiatives. I strongly suspect that Highsmith et al have spent more time closer to "the C Suite" than Itamar, and conversely he's spent more time nearer the coal-face. So I'm not really knocking what they're saying too much: we're all still capitalists (for now), so working out where the budget for all those expensive engineers, and designers (and Data Centres) is *very* important.

Highsmith et al model uncertainty in terms of "Bets". They define a "Bet" as "hypotheses of value that the organisation believes will help it realise a goal".

So already I don't like this. "Hypothesis" is a scientific term that's not about "belief", per-se. It's an acknowledgment that there's *something we don't understand*. This in turn comes with a degree to which you don't understand it, and that degree determines the type of experiment you need to do next.

I understand *why* Highsmith and co have taken this approach - it's a perfectly healthy understanding that we don't know the future, and that the lack of friction in the Digital Realm helps us discover it methodically as we go. Highsmith himself sums this up wonderfully: "following a plan produces the product you intended, just not the product that you need". Your original intention was wrong, because you didn't know enough.

Highsmith et al suggest a template for a "hypothesis": "We believe that by *doing this thing* we will *cause this valuable outcome*. We will validate this idea by *performing this action*." An example they give relates to the Goal of "being the market leader of retirement solutions", where the Bet is defined as: "We believe that *by providing low-cost advice to people prior to retirement*, we can *help customers better achieve their retirement goals*. We will validate this idea by *introducing a specialist advisor*..."

For my money this concept of "Bets" is a broken metaphor. It's clunky, and it doesn't really represent the real nature of a "bet". You bet on something when you're presented the odds of something happening and you reckon they've been worked out wrong. At this point, you are *certain* that you understand the probabilities better that the person presenting the odds, and you're prepared to risk your money on the fact the chance is greater than your opponent believes them to be. It's got nothing to do with hypothesising, really, which is about learning more about things that are only partially understood. Being humble, in other words, and admitting that you don't know everything. When gambling, both protagonists each think they already know more about a given situation than the other. GIST's "Confidence Levels" encapsulate this situation a lot better than "betting", for me.

When it comes to measuring whether or not a "Bet" is paying off, though, Highsmith et al get back onto slightly less shakey ground, with their concept of "Measures of Success". Again, fundamental difference with GIST, here - in Highsmith et al's world, measurement is kept separate from the organisational structure, while it GIST it's a fundamental part of it. There's some good stuff in the Measures of Success framework about Leading and Lagging Indicators: Goals (at the higher level) tend to be measured by Lagging ones: metrics that take a while to pay off, such as "losing weight" or "increasing earnings", while Leading indicators (things you can measure immediately, such as "amount you'r eating" or "revenue you're bringing in") are better measures at the lower "Initiative" level. The idea is, of course, that you want changes in the lower level to impact those higher up in a good way.

## Prototyping to Learn

Bob Moesta has an entirely different approach to uncertainty, though it starts from the same position of "we need to deliver outcomes not outputs", and that the outcome should be value to customers in the market. In Bob's world, that value relates explicitly to "struggling moments", i.e. the points at which (in JTBD parlance), potential customers are most likely to want to hire your solution. However, this is only one of five "bedrock skills of an entrepreneur" (I've highlighted the two that most relate to uncertainty):

1. Empathetic Perspective
2. Uncovering Demand (this is the Struggling Moment one)
3. *Causal Structures*
4. *Prototyping to Learn*
5. Making Tradeoffs

Bob's a born-and-bred engineer, who was mentored by some of the key figures in the Lean world, so he's a big systems thinker. As such, he's got a systematic and statistical approach to dealing with uncertainty, based on understanding the problem, the potential solution, its context and the causes and effects of the phenomena that relate to it. In particular, he's big on understanding the contextual factors that impact a system, and working out which ones can be controlled and which one act as noise. Noise factors have an impact, but are either completely uncontrollable, like the weather, for example, or public opinion. Or noise just costs too much to control: the example Bob uses is the level of humidity in a factory, we could control it but would have to install hugely expensive air-conditioning.

Once you've worked that model out (that's where *Empathetic Perspective* is needed), you can use *Prototyping to Learn* to quickly cover *all* the variables in your model in the smallest number of experiments possible, and use statistics to get the balance of your solution correct. The example Bob uses in his Learning To Build book is a optimising the design of a catapult based on distance and accuracy of the projectile, using the control factors of the amount of elastic you use, the angle of the throwing arm and the type of projectile, with noise factors relating to distance, accuracy and cost . Then one orthogonal array based on how variance in the three control factors affect the noise factors can be used to work out how to explore the whole solution space in the smallest number of steps. (E.g. if we use more elastic then the distance potentially increases, but it costs more, type of thing).

This is the major difference in Bob's approach to the other two - Bob is *very sniffy* about A/B test baby-steps: the "... has this one change to this one variable moved the needle in the right direction?" approach of the other two frameworks, as he considers learning in this way to be inefficient when we can explore the solution space more efficiently and methodically.

On paper this makes a lot of sense, but in reality *we can only go for a big bang test suite if we're certain we've got the right model of the system*, and in the Digital Realm, such certainties are rare. They're not completely unheard of, though. For instance, we might need to consider the best design for a technically complex sub-system in our solution somewhere. At this stage, Bob's approach to modelling that sub-system and working out the best balance of control factor inputs (e.g. patterns within the code that a containerised process runs) to minimise the noise we're most worried about (e.g. getting sufficient throughput of the data that the process is required to chew through) would be ideal.

There's another intriguing idea, too - perhaps the Business Model Canvas could be representative of the sort of system Bob suggests we model? We can control our Revenue and Cost Models, and Delivery Channels, for example, but Customer Relationships and the nature of Customer Segments are both a bit more "noisy". So rather than iterating through models quickly, dealing with one variable at a time, we might explore a whole heap of models in one go? We'd need to simulate the product's behaviour in the market at that point, though... This is starting to sound like the basis of a whole PhD, though, and I've already got one of those, thanks. If someone else wants to do it then fire away.

Fundamentally, though, all of this "Bobbishness" represents a big stride away from small-a-agility, as it all tends towards "big design up-front". I believe this to be evident from the work Bob's done in the Digital Realm (quite a bit of very successful stuff, with Basecamp in particular), which tends to lean more towards the JTBD aspects of his expertise and encouraging people to align their digital products up with Struggling Moments and increase the desirability of their products more methodically. Bob has a very optimistic attitude to working out costs, too - pretty much claiming that they'll be "self-evident" in a lot of cases - a claim which I think rings a lot truer when we're building real things with actual materials and production lines, but which is definitely NOT the case in the Digital Realm, where the de-facto approach seems to be: "we'll work out what it costs when Jeff Bezos sends us the bill".

To sum up, as I say, I don't like the gambling metaphor much, but just to prove how big a hypocrite I can be, I'm going to use it now. Itamar and Highsmith et al are playing Blackjack. They're turning cards over one at a time, then reacting to the next card. The Digital Realm and small-a-agility lets them turn the cards over quickly, but a lot of cards have to be turned over, potentially, before either of them have won enough to leave the table.

Bob is playing roulette: so he's covering the table in bets and using a smaller number of rolls to try and beat the house. However - he's not *really* playing roulette, as that would suggest he's working on pure chance, whereas what he's *actually* doing is working out *exactly* where to place his bets to win money every time. So this is where the whole gambling metaphor starts to fall over. Told you I didn't like it.

Bob's *actual* gamble is on whether the model of the system that's being used to work out where to place the bets is a good one or not. That's a lot less of a gamble in the Real World of physics, gravity, catapults etc, but much more of one in the Digital Realm, where there's nothing physically stopping us from modelling anything anyhow.

## Assumption Mapping

Assumption Mapping is a technique that's covered in the third of the Strategyzer books: [Testing Business Ideas](https://www.strategyzer.com/library/testing-business-ideas-book), by David J. Bland and Alex Osterwalder. Simply put, your autonomous, empowered team (see below) should get together regularly, review the latest state of the Business Model Canvas, and capture the assumptions that are being made: i.e. the things that the team believes to be true, but with varying degrees of proof and hence levels of confidence.

Once you've got a decent list of all the stuff you know you don't know, you slap the items from the list onto a 2x2 matrix, with the degree to which you're guessing on the x axis (from known to unknown), and the degree of importance to the success of the Business Model on the y axis (from unimportant at the bottom to important at the top). This will leave you with a top-right quadrant of "important things that we don't know much about", for you to focus on.

Another neat aspect of the way it's described in the book is that it maps back onto Marty Cagan's set of "Viability, Feasibility and Usability / Desirability": indeed the book constitutes a bit of an upgrade to the Business Model Canvas overall by providing that mapping:

* **Usability / Desirability** maps onto the top right-hand side of canvas, covering Customer / Market segments, Customer Relationships and Channels.
* **Feasibility** maps onto the left-hand side, so Key Activities, Key Resources and Key Partners. The "how are we going to build this thing? And who are we going to need to get help from?... stuff.
* **Viability** maps onto the bottom of the canvas - Revenue and Cost models (remember kids, value = Benefits - Costs and all that).

Then all your assumptions can also be categorised by the same three dimensions, which helps break up the work of testing them nicely (e.g. the Engineer can focus upon the least known, most important Feasibility assumptions, the Product Designer can focus on the Usability / Desirability ones, and the Product Manager on the Viability ones).

Where I deviate a little from all this is that "Value Propositions" are lumped in with Usability and Desirability, when I would suggest they should really be the focus of the whole team at all times. Indeed I'd state that Strategyzer think this, too, or else they wouldn't have dedicated a whole book to them. And remember, they're a direct link back to the whole of "Jobs To Be Done", as well. So I would suggest getting into Bob Moesta's stuff while learning about the Value Proposition Canvas, too.
